<div align="center">

<h1>AMD_Open_Robotics_Hackathon_2025_PackingMan</h1>

<h3>Dual-Arm Packing with Imitation Learning on AMD Instinctâ„¢ GPUs</h3>

**Team Nyap IV**  
Masakazu Sueyoshi Â· Hiroki Kyono  Â· Kosuke Tokuda

<br>

<sup>AMD Open Robotics Hackathon 2025</sup>

<br><br>

<!-- ğŸ”— ã“ã“ã ã‘å„è‡ªã®ã‚‚ã®ã«å·®ã—æ›¿ãˆã¦ãã ã•ã„ -->
<a href="https://huggingface.co/datasets/<HF_USERNAME>/<MISSION1_DATASET_ID>">
  <img src="https://img.shields.io/badge/HuggingFace-Mission1_Dataset-orange" alt="Mission1 Dataset">
</a>
<a href="https://huggingface.co/datasets/<HF_USERNAME>/<MISSION2_DATASET_ID>">
  <img src="https://img.shields.io/badge/HuggingFace-Mission2_Dataset-orange" alt="Mission2 Dataset">
</a>
<a href="https://huggingface.co/<HF_USERNAME>/<MISSION1_MODEL_ID>">
  <img src="https://img.shields.io/badge/HuggingFace-Mission1_Model-blue" alt="Mission1 Model">
</a>
<a href="https://huggingface.co/<HF_USERNAME>/<MISSION2_MODEL_ID>">
  <img src="https://img.shields.io/badge/HuggingFace-Mission2_Model-blue" alt="Mission2 Model">
</a>

</div>

---

## ğŸ§¾ Title
**AMD_RoboticHackathon2025-PackingMan**

---

## ğŸ‘¥ Team

**Team Name:** Nyap IV  

**Members:**

- Masakazu Sueyoshi  
- Kosuke Tokuda  
- Hiroki Kyono  

---
## ğŸ“ Summary

**PackingMan is an automatic packing system that targets the â€œlast human stepâ€ in modern e-commerce logistics: putting items into cardboard boxes.**

### Background

### Problem

### Proposed method


---

## ğŸ—‚ Repository Structure

The repository follows the official template:

```terminal
AMD_Robotics_Hackathon_2025_PackingMan/
â”œâ”€â”€ README.md
â”œâ”€â”€ mission1
â”‚   â”œâ”€â”€ code
â”‚   â”‚   â””â”€â”€ <code and script>
â”‚   â””â”€â”€ wandb
â”‚       â””â”€â”€ <latest run directory copied from wandb of your training job>
â””â”€â”€ mission2
    â”œâ”€â”€ code
    â”‚   â””â”€â”€ <code and script>
    â””â”€â”€ wandb
        â””â”€â”€ <latest run directory copied from wandb of your training job>
```

- `mission1/code`  
  Code & scripts for the unified hackathon task (training, evaluation, inference).

- `mission1/wandb`  
  Weights & Biases logs for the **last successful** Mission 1 training run.

- `mission2/code`  
  Code for the PackingMan task (teleoperation, data recording, training, and deployment).

- `mission2/wandb`  
  Weights & Biases logs for the **last successful** Mission 2 training run.

---

## ğŸ“¦ W&B Run Layout

The `latest-run` directory generated by W&B looks like this:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
â”œâ”€â”€ debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
â”œâ”€â”€ debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
â”œâ”€â”€ latest-run -> run-20251029_063411-tz1cpo59
â””â”€â”€ run-20251029_063411-tz1cpo59
    â”œâ”€â”€ files
    â”‚   â”œâ”€â”€ config.yaml
    â”‚   â”œâ”€â”€ output.log
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ wandb-metadata.json
    â”‚   â””â”€â”€ wandb-summary.json
    â”œâ”€â”€ logs
    â”‚   â”œâ”€â”€ debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    â”‚   â”œâ”€â”€ debug-internal.log
    â”‚   â””â”€â”€ debug.log
    â”œâ”€â”€ run-tz1cpo59.wandb
    â””â”€â”€ tmp
        â””â”€â”€ code
```

### â— Important Notes

1. `latest-run` is a **symbolic link**.  
   Please make sure to copy the **actual target directory** (e.g., `run-20251029_063411-tz1cpo59`) with **all subdirectories and files** into `mission1/wandb/` and `mission2/wandb/`.

2. For each Mission, upload only the W&B directory of your **last successful pre-trained model**.

---

## ğŸš€ How to Reproduce

Below is a minimal, end-to-end workflow.  
You can literally copyâ€“paste this section and only change IDs / ports / camera configs as needed.

### 0. Environment Setup

We use Python 3.11, LeRobot, and AMD Instinctâ„¢ MI300X (CUDA-compatible interface).

```bash
# Create environment (example; adjust to your preference)
conda create -n amd_packingman python=3.11
conda activate amd_packingman

# Install LeRobot with Ï€0 / ACT support
git clone https://github.com/huggingface/lerobot.git
cd lerobot
git checkout -b v0.4.1 v0.4.1
pip install -e ".[pi]"   # includes Ï€0 dependencies

cd ..
```

You can then place this repository anywhere, e.g.:

```bash
git clone https://github.com/<YOUR_GITHUB_ACCOUNT>/AMD_Robotics_Hackathon_2025_PackingMan.git
cd AMD_Robotics_Hackathon_2025_PackingMan
```

---

## ğŸ§ª Mission 1 â€“ Unified Task

### 1-1. Data Collection

```bash
lerobot-record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM1 \
  --robot.cameras="{ front: {type: opencv, index_or_path: 0, width: 1280, height: 720, fps: 10} }" \
  --robot.id=mission1_follower_arm \
  --teleop.type=so101_leader \
  --teleop.port=/dev/ttyACM0 \
  --teleop.id=mission1_leader_arm \
  --dataset.repo_id=<HF_USERNAME>/<MISSION1_DATASET_ID> \
  --dataset.single_task='Mission1 Unified Task' \
  --display_data=false
```

This will:

- Stream images from the `front` camera.
- Use the leader arm as teleoperation input.
- Record demonstrations into a Hugging Face dataset `<HF_USERNAME>/<MISSION1_DATASET_ID>`.

---

### 1-2. Training

```bash
lerobot-train \
  --dataset.repo_id=<HF_USERNAME>/<MISSION1_DATASET_ID> \
  --batch_size=128 \
  --steps=5000 \
  --output_dir=outputs/train/mission1 \
  --job_name=mission1_packingman \
  --policy.type=act \
  --policy.device=cuda \
  --policy.push_to_hub=true \
  --policy.repo_id=<HF_USERNAME>/<MISSION1_MODEL_ID> \
  --wandb.enable=true
```

- `batch_size` and `steps` are set for MI300X-class GPUs.  
  You can increase either if you have more data or want longer training.
- The final trained policy is uploaded to **Hugging Face** under `<MISSION1_MODEL_ID>`.

Copy the corresponding W&B `run-XXXX` directory into:

```text
mission1/wandb/
```

---

### 1-3. Inference / Evaluation

```bash
lerobot-act \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM1 \
  --robot.cameras="{ front: {type: opencv, index_or_path: 0, width: 1280, height: 720, fps: 10} }" \
  --robot.id=mission1_follower_arm \
  --policy.repo_id=<HF_USERNAME>/<MISSION1_MODEL_ID> \
  --policy.device=cuda
```

This will load the trained Mission 1 policy from Hugging Face and execute it on the real robot.

---

## ğŸ“¦ Mission 2 â€“ PackingMan (Custom Task)

PackingMan focuses on **picking multiple objects and packing them into a box**, potentially with **dual-arm coordination**.

### 2-1. Data Collection (Teleoperation)

```bash
lerobot-record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM1 \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 1280, height: 720, fps: 10} }" \
  --robot.id=packingman_follower_arm \
  --teleop.type=so101_leader \
  --teleop.port=/dev/ttyACM0 \
  --teleop.id=packingman_leader_arm \
  --dataset.repo_id=<HF_USERNAME>/<MISSION2_DATASET_ID> \
  --dataset.single_task='PackingMan â€“ Packing Objects into Box' \
  --display_data=false
```

If you extend to **dual-arm control**, you can follow the hackathonâ€™s dual-arm teleop configuration (left/right leader and follower arms) and log a combined state/action vector in the dataset.

---

### 2-2. Training on AMD Instinctâ„¢ GPU

```bash
lerobot-train \
  --dataset.repo_id=<HF_USERNAME>/<MISSION2_DATASET_ID> \
  --batch_size=128 \
  --steps=5000 \
  --output_dir=outputs/train/packingman \
  --job_name=packingman_dualarm \
  --policy.type=act \
  --policy.device=cuda \
  --policy.push_to_hub=true \
  --policy.repo_id=<HF_USERNAME>/<MISSION2_MODEL_ID> \
  --wandb.enable=true
```

- For dual-arm policies, the **action dimension** corresponds to the concatenation of both armsâ€™ joint commands.
- Since MI300X has plenty of memory, you can experiment with **larger batch sizes** or **more steps** if time allows.

Copy the W&B `run-XXXX` directory of the **last successful training** into:

```text
mission2/wandb/
```

---

### 2-3. Running PackingMan Policy

```bash
lerobot-act \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM1 \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 1280, height: 720, fps: 10} }" \
  --robot.id=packingman_follower_arm \
  --policy.repo_id=<HF_USERNAME>/<MISSION2_MODEL_ID> \
  --policy.device=cuda
```

This command:

- Loads the trained PackingMan policy from Hugging Face.
- Runs it on your follower arm with the same camera setup used during recording.
- Allows you to reproduce the **final demo** shown during the hackathon.

---

## ğŸŒ Delivery URLs

Please fill in these URLs when everything is uploaded (structureã¯ãã®ã¾ã¾ã‚³ãƒ”ãƒšã§OKã§ã™):

```text
Dataset (Mission 1):
https://huggingface.co/datasets/<HF_USERNAME>/<MISSION1_DATASET_ID>

Dataset (Mission 2 â€“ PackingMan):
https://huggingface.co/datasets/<HF_USERNAME>/<MISSION2_DATASET_ID>

Model (Mission 1):
https://huggingface.co/<HF_USERNAME>/<MISSION1_MODEL_ID>

Model (Mission 2 â€“ PackingMan):
https://huggingface.co/<HF_USERNAME>/<MISSION2_MODEL_ID>
```

---

## ğŸ“ Changelog

- 2025-XX-XX: Initial release of code, models, and datasets for AMD Open Robotics Hackathon 2025.
- 2025-XX-XX: Added dual-arm PackingMan support and updated README.
- 2025-XX-XX: Cleaned W&B logs and finalized submission structure.

*(æ—¥ä»˜ã¯æå‡ºå‰ã«æ›¸ãæ›ãˆã¦ãã ã•ã„)*

---

## ğŸ™ Acknowledgement

- This work was conducted as part of **AMD Open Robotics Hackathon 2025**.
- We use:
  - **LeRobot** (Hugging Face) for imitation learning infrastructure.
  - **Hugging Face Hub** for hosting datasets and models.
  - **Weights & Biases** for experiment tracking.
- Special thanks to the organizers and supporting engineers for providing hardware, software templates, and technical guidance.

---

## ğŸ“œ License

Please choose and declare a license appropriate for your project (e.g., MIT, Apache-2.0):

```text
MIT License
```

*(å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„)*

---
